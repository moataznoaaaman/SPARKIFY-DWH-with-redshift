# SPARKIFY DWH WITH AWS ![This is a alt text.](https://img.shields.io/badge/Amazon_AWS-FF9900?style=for-the-badge&logo=amazonaws&logoColor=white)

### Introduction and business problem
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. To achieve that with low barier to entry, flexiblity of any design changes, and scability and elasticity redshift, s3, IAM roles, and boto3 SDK where used.


### redshift cluster architecture 
#### Identifiers & Credentials
* DWH_CLUSTER_IDENTIFIER : sparkify
* DWH_IAM_ROLE_NAME : readfroms3
* HOST : 'sparkify.cqa4vd5eex7x.us-west-2.redshift.amazonaws.com'
* DB_NAME : sparkifydb
* DB_USER : moataz
* DB_PASSWORD : 
* DB_PORT : 5439

#### Hardware
* DWH_CLUSTER_TYPE : multi-node
* DWH_NUM_NODES : 4
* DWH_NODE_TYPE : dc2.large

### ETL process
The data, which is in json format, is first extracted from an S3 bucket into two staging tables "events, and songs", the data is then transformed from there rwa form to the needed for for the star schema of the dimensional model that will be used, and finally the data is loaded to the dimensional model.

### Data
#### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
#### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.
#### Data extraction
To enable efficent loading of data, the data was drilled down to small files which is aimed to enable fast extraction using redshift COPY command which will use its MPP to load the data.
### SQL tables
#### Staging tables
1. staging_events : this table is responsible for storing the events data extracted from the s3 bucket the columns names and orders are the same as the fields in the JSON files.
1. staging_events : this table is responsible for storing the songs data extracted from the s3 bucket the columns names and orders are the same as the fields in the JSON files.

#### Star schema tables
##### Fact Table
songplays : records in event data associated with song plays i.e. records with page "NextSong"
Columns : 
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
##### Dimension Tables
* users : represents the users in the app, 
columns :
user_id, first_name, last_name, gender, level

* songs : represents the songs in music database, columns : 
song_id, title, artist_id, year, duration

* artists : represents the artists in music database, columns: 
artist_id, name, location, lattitude, longitude

* time : represents the timestamps of records in songplays broken down into specific time units, columns: 
start_time, hour, day, week, month, year, weekday


### implementation
##### cluster.ipynb
This notebook is responsible for creating an IAM role and attaching the appropiate policy for the cluster to be able to read the data from the s3 bucket. More importantly, its responsible for creating the redshift cluster. It also contain functions for deleating the resources and veryfing the data. 

##### sqlqueries.py
This script contain the sql queries that will be executed to drop/create tables, extract the data to the staging tables, transform and load the data to the star schema tables. 

##### createtables.py
This script will execute the queries responsible fro droping/creating tables in the redshift cluster. 

##### etl.py
This script is responsible for executing the sql queries that will extract the data from the bucket, transform and load the data to the star schema. 

##### dwh.cfg
This is a configuration file that contains all the data used to init the cluster, db, and the role as well as connecting to them.

### Running the project
1. adjust the config file to you parameters
1. run the cluster notebook to create the role and the cluster
1. run the create tables script to create the staging and star achema tables
1. finally, run the etl script to get the star schema ready